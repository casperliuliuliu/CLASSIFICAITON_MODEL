{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "217abcf7-6f54-413e-a573-42ab9d6c37df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa41ce46-b36e-488b-9724-2e7d3356be65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = 'output.txt'\n",
    "# f = open(path, 'a')\n",
    "# # f.write('Hello World\\n')\n",
    "# time.sleep(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a96c077b-21df-4519-a4d0-3dd13c5ca2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# timestamp = datetime.datetime.now().strftime(\"[%Y-%m-%d %H:%M:%S] \")\n",
    "# f.write(timestamp+'Hello World\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c790be2-b8d8-4665-985d-c067a1654d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.array([0.5, 0.5, 0.5])\n",
    "std = np.array([0.25, 0.25, 0.25])\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dbee27a-b82d-4e55-b5b3-631cbdf457fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"E:/casper/raw_data_training\"\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                             shuffle=True, num_workers=0)\n",
    "              for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f163cbaf-896d-423b-8917-250c76be6c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IgA', 'MN']\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(class_names)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e6a4efb-eedf-4b02-a059-9e90f8160f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "# imshow(out, title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68f570ed-7174-41d7-8648-92dfb2a0c5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # timestamp = datetime.datetime.now().strftime(\"[%Y-%m-%d %H:%M:%S] \")\n",
    "        # f.write(timestamp+'\\n')\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        # f.write('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        # f.write('\\n')\n",
    "        # f.write('-' * 10)\n",
    "        # f.write('\\n')\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "            # f.write('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "            #     phase, epoch_loss, epoch_acc))\n",
    "            # f.write('\\n')\n",
    "            \n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "        # f.write('\\n')\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a80eecdd-e35a-4a35-97dc-97d64f516e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112\n"
     ]
    }
   ],
   "source": [
    "#### Finetuning the convnet ####\n",
    "# Load a pretrained model and reset final fully connected layer.\n",
    "\n",
    "# from efficientnet_pytorch import EfficientNet\n",
    "# from torch import nn\n",
    "# model = EfficientNet.from_pretrained('EfficientNet_V2_S_Weights')\n",
    "model = models.efficientnet_v2_s()\n",
    "print(len(dir(model)))\n",
    "# num_ftrs = model.fc.in_features\n",
    "num_ftrs = model.classifier[1].in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7167cc05-856d-4f6d-a586-2c239578c866",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier[1] = nn.Linear(num_ftrs, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "589544db-9a2d-4259-b0f4-bbcb44b7e2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63c0f7e2-d702-4372-bdf2-d715e5aee2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02210440-1eb8-4dcb-af8a-ecedcec46660",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2a87663-6061-498e-9b23-405f4cb3eeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "133f2f47-7a98-417f-adcc-12e181f97419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1\n",
      "----------\n",
      "train Loss: 0.6439 Acc: 0.6315\n",
      "val Loss: 0.6907 Acc: 0.5971\n",
      "\n",
      "Epoch 1/1\n",
      "----------\n",
      "train Loss: 0.6164 Acc: 0.6569\n",
      "val Loss: 0.6922 Acc: 0.5569\n",
      "\n",
      "Training complete in 6m 26s\n",
      "Best val Acc: 0.597080\n"
     ]
    }
   ],
   "source": [
    "model = train_model(model, criterion, optimizer, step_lr_scheduler, num_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0232c968-f2d8-47b1-847a-346c8b075ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VLSI\\anaconda3\\envs\\casper_env\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_V2_S_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112\n",
      "Epoch 0/1\n",
      "----------\n",
      "train Loss: 0.5971 Acc: 0.6716\n",
      "val Loss: 0.7378 Acc: 0.5540\n",
      "\n",
      "Epoch 1/1\n",
      "----------\n",
      "train Loss: 0.5676 Acc: 0.6978\n",
      "val Loss: 0.6454 Acc: 0.6445\n",
      "\n",
      "Training complete in 5m 11s\n",
      "Best val Acc: 0.644526\n"
     ]
    }
   ],
   "source": [
    "model = models.efficientnet_v2_s(weights = True)\n",
    "print(len(dir(model)))\n",
    "num_ftrs = model.classifier[1].in_features\n",
    "model.classifier[1] = nn.Linear(num_ftrs, 2)\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "model = train_model(model, criterion, optimizer, step_lr_scheduler, num_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4aa0867d-d424-4615-b537-f9037ae0e4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1\n",
      "----------\n",
      "train Loss: 0.6138 Acc: 0.6596\n",
      "val Loss: 0.7523 Acc: 0.5650\n",
      "\n",
      "Epoch 1/1\n",
      "----------\n",
      "train Loss: 0.6052 Acc: 0.6711\n",
      "val Loss: 345.0967 Acc: 0.5839\n",
      "\n",
      "Training complete in 5m 8s\n",
      "Best val Acc: 0.583942\n"
     ]
    }
   ],
   "source": [
    "#### ConvNet as fixed feature extractor ####\n",
    "# Here, we need to freeze all the network except the final layer.\n",
    "# We need to set requires_grad == False to freeze the parameters so that the gradients are not computed in backward()\n",
    "model_conv = models.efficientnet_v2_s(weights = True)\n",
    "for param in model_conv.parameters():\n",
    "    # print(param)\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "\n",
    "num_ftrs = model_conv.classifier[1].in_features\n",
    "model_conv.classifier[1] = nn.Linear(num_ftrs, 2)\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized as\n",
    "# opposed to before.\n",
    "optimizer_conv = optim.SGD(model_conv.classifier[1].parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)\n",
    "\n",
    "model_conv = train_model(model_conv, criterion, optimizer_conv,\n",
    "                         exp_lr_scheduler, num_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "513b00b4-c735-4001-861f-2f437bffd21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 0.6063 Acc: 0.6632\n",
      "val Loss: 0.6708 Acc: 0.6277\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 0.5615 Acc: 0.7020\n",
      "val Loss: 0.6289 Acc: 0.6854\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 0.5502 Acc: 0.7144\n",
      "val Loss: 0.6031 Acc: 0.7058\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 0.5390 Acc: 0.7245\n",
      "val Loss: 0.5609 Acc: 0.7292\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 0.5196 Acc: 0.7410\n",
      "val Loss: 0.5471 Acc: 0.7431\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 0.5123 Acc: 0.7431\n",
      "val Loss: 0.5257 Acc: 0.7591\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 0.5080 Acc: 0.7530\n",
      "val Loss: 0.5610 Acc: 0.7292\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 0.4991 Acc: 0.7549\n",
      "val Loss: 0.5373 Acc: 0.7511\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 0.4959 Acc: 0.7553\n",
      "val Loss: 0.5238 Acc: 0.7547\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 0.4904 Acc: 0.7688\n",
      "val Loss: 0.5699 Acc: 0.7285\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 0.4985 Acc: 0.7563\n",
      "val Loss: 0.5196 Acc: 0.7555\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 0.4921 Acc: 0.7615\n",
      "val Loss: 0.5374 Acc: 0.7504\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 0.4868 Acc: 0.7655\n",
      "val Loss: 0.4933 Acc: 0.7759\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 0.4909 Acc: 0.7576\n",
      "val Loss: 0.4923 Acc: 0.7788\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 0.4872 Acc: 0.7682\n",
      "val Loss: 0.5333 Acc: 0.7445\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 0.4902 Acc: 0.7630\n",
      "val Loss: 0.5112 Acc: 0.7650\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 0.4877 Acc: 0.7639\n",
      "val Loss: 0.5154 Acc: 0.7584\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 0.4898 Acc: 0.7646\n",
      "val Loss: 0.4680 Acc: 0.7818\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 0.4839 Acc: 0.7673\n",
      "val Loss: 0.4941 Acc: 0.7774\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 0.4832 Acc: 0.7689\n",
      "val Loss: 0.4995 Acc: 0.7759\n",
      "\n",
      "Training complete in 51m 54s\n",
      "Best val Acc: 0.781752\n"
     ]
    }
   ],
   "source": [
    "model_v2 = models.efficientnet_v2_s(weights = True)\n",
    "num_ftrs = model_v2.classifier[1].in_features\n",
    "model_v2.classifier[1] = nn.Linear(num_ftrs, 2)\n",
    "model_v2 = model_v2.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_v2.parameters(), lr=0.001)\n",
    "step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "model_v2 = train_model(model_v2, criterion, optimizer, step_lr_scheduler, num_epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94b33155-5577-46ef-8aa7-55a9a41e1b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 0.6082 Acc: 0.6666\n",
      "val Loss: 13538.6251 Acc: 0.5496\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 0.6059 Acc: 0.6675\n",
      "val Loss: 3807.3322 Acc: 0.5518\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 0.5986 Acc: 0.6784\n",
      "val Loss: 919.4916 Acc: 0.6088\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 0.6119 Acc: 0.6673\n",
      "val Loss: 12685.1134 Acc: 0.5460\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 0.6095 Acc: 0.6719\n",
      "val Loss: 0.6507 Acc: 0.6401\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 0.6084 Acc: 0.6749\n",
      "val Loss: 5710.3567 Acc: 0.5964\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 0.6137 Acc: 0.6693\n",
      "val Loss: 843.4593 Acc: 0.5861\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 0.5966 Acc: 0.6747\n",
      "val Loss: 2003.4994 Acc: 0.5723\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 0.5945 Acc: 0.6767\n",
      "val Loss: 37.2705 Acc: 0.5715\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 0.5883 Acc: 0.6795\n",
      "val Loss: 0.6783 Acc: 0.5985\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 0.5895 Acc: 0.6792\n",
      "val Loss: 9.5115 Acc: 0.6088\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 0.5909 Acc: 0.6872\n",
      "val Loss: 0.6838 Acc: 0.6029\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 0.5843 Acc: 0.6870\n",
      "val Loss: 0.6923 Acc: 0.5759\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 0.5832 Acc: 0.6845\n",
      "val Loss: 78.7742 Acc: 0.5445\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 0.5825 Acc: 0.6824\n",
      "val Loss: 70.5992 Acc: 0.5891\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 0.5821 Acc: 0.6927\n",
      "val Loss: 0.7208 Acc: 0.5693\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 0.5900 Acc: 0.6849\n",
      "val Loss: 2158.3277 Acc: 0.5613\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 0.5829 Acc: 0.6827\n",
      "val Loss: 0.6872 Acc: 0.6080\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 0.5894 Acc: 0.6816\n",
      "val Loss: 1.3773 Acc: 0.5708\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 0.5783 Acc: 0.6903\n",
      "val Loss: 47.2012 Acc: 0.5964\n",
      "\n",
      "Training complete in 51m 25s\n",
      "Best val Acc: 0.640146\n"
     ]
    }
   ],
   "source": [
    "#### ConvNet as fixed feature extractor ####\n",
    "# Here, we need to freeze all the network except the final layer.\n",
    "# We need to set requires_grad == False to freeze the parameters so that the gradients are not computed in backward()\n",
    "model_conv = models.efficientnet_v2_s(weights = True)\n",
    "for param in model_conv.parameters():\n",
    "    # print(param)\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "\n",
    "num_ftrs = model_conv.classifier[1].in_features\n",
    "model_conv.classifier[1] = nn.Linear(num_ftrs, 2)\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized as\n",
    "# opposed to before.\n",
    "optimizer_conv = optim.SGD(model_conv.classifier[1].parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)\n",
    "\n",
    "model_conv = train_model(model_conv, criterion, optimizer_conv,\n",
    "                         exp_lr_scheduler, num_epochs=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
